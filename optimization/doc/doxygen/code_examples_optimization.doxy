/**
\page code_examples_optimization Short Code Examples: Optimization

On this page, we look at an example application of the SGPP::optimization
module.
The example is written in C++.
However, as the module is also incorporated in pysgpp, one can also use
the Python interface.
Via the Java interface (jsgpp), the module should be usable in MATLAB
if compiled without support for Armadillo and UMFPACK
(see below for instructions).

\section code_examples_optimization_cpp C++

Below, you can find the whole C++ example.
The example minimizes a specific function \f$f\colon [0, 1]^2 \to \mathbb{R}\f$
using sparse grids and the SGPP::optimization module.
For the basic structure of the optimization, please see
\ref module_optimization.

\include c++_example.cpp

We will now discuss the example in some detail line by line.

\dontinclude c++_example.cpp
\until sgpp_optimization.hpp

We need to include the headers for the SGPP::base and the SGPP::optimization
module.

\until };

The function \f$f\colon [0, 1]^d \to \mathbb{R}\f$ to be minimized
is called "objective function" and has to derive from
SGPP::optimization::ObjectiveFunction.
In the constructor, we give the dimensionality of the domain
(in this case \f$d = 2\f$).
The eval method evaluates the objective function and returns the function
value \f$f(\vec{x})\f$ for a given point \f$\vec{x} \in [0, 1]^d\f$.
The clone method returns a std::unique_ptr to a clone of the object
and is used for parallelization (in case eval is not thread-safe).

\until f, grid, N, gamma

First, we define a grid with modified B-spline basis functions and
an iterative grid generator, which can generate the grid adaptively.

\until }

With the iterative grid generator, we generate adaptively a sparse grid.

\until }

Then, we hierarchize the function values to get hierarchical B-spline
coefficients of the B-spline sparse grid interpolant
\f$\tilde{f}\colon [0, 1]^d \to \mathbb{R}\f$.

\until ftX0

We define the interpolant \f$\tilde{f}\f$ and its gradient
\f$\nabla\tilde{f}\f$ for use with the gradient method (steepest descent).
Of course, one can also use other optimization algorithms from
SGPP::optimization::optimizer.

\until ft(x0)

The gradient method needs a starting point.
We use a point of our adaptively generated sparse grid as starting point.
More specifically, we use the point with the smallest
(most promising) function value and save it in x0.

\until ft(xOpt)

We apply the gradient method and print the results.

\until }

For comparison, we apply the classical gradient-free Nelder-Mead method
directly to the objective function \f$f\f$.

The example program outputs the following results:
\verbinclude all_example.output.txt

We see that both the gradient-based optimization of the smooth sparse grid
interpolant and the gradient-free optimization of the objective function
find reasonable approximations of the minimum, which lies at
\f$(3\pi/16, 3\pi/14) \approx (0.58904862, 0.67319843)\f$.

To compile and run the C++ example, you need to set some
compiler/linker flags and you also have to adjust the environment
variable \c LD_LIBRARY_PATH accordingly
(see \ref code_examples_quick_start).

\section code_examples_optimization_python Python

The C++ code can be translated into the following Python code:
\include python_example.py

To compile run the Python example, you need to adjust the environment
variables \c LD_LIBRARY_PATH and \c PYTHONPATH accordingly
(see \ref code_examples_quick_start).
When run, the example gives (nearly) the same output as the C++ example.

\section code_examples_optimization_java Java

The C++ code can be translated into the following Java code:
\include java_example.java

The Java and MATLAB examples use this external class:
\include ExampleFunction.java

To run the Java example, you need to adjust the environment
variable \c LD_LIBRARY_PATH accordingly
(see \ref code_examples_quick_start).
When run, the example gives (nearly) the same output as the C++ example.

\section code_examples_optimization_matlab MATLAB

The C++ code can be translated into the following MATLAB code:
\include matlab_example.m

Please note that in order to get SGPP::optimization to work with MATLAB,
you have to disable support for Armadillo and UMFPACK when compiling SG++,
i.e. set USE_ARMADILLO and USE_UMFPACK to "no".
This is due to incompatible BLAS and LAPACK libraries
of Armadillo/UMFPACK and MATLAB
(MATLAB uses instead MKL versions of LAPACK and BLAS
with different pointer sizes of 64 bits).
You can somehow override MATLAB's choice of libraries with
the environmental variables BLAS_VERSION and LAPACK_VERSION,
but this is strongly discouraged as MATLAB itself may produce
wrong results (e.g., det [1 2; 3 4] = 2).

To run the MATLAB example, you first need to compile to \c ExampleFunction
into a \c .jar file:
\code
javac -cp .:/path/to/SGpp/trunk/lib/jsgpp/jsgpp.jar ExampleFunction.java
jar -cf ExampleFunction.jar ExampleFunction.class
\endcode

Second, you need to adjust \c /path/to/MATLAB/toolbox/local/librarypath.txt
to include the \c /path/to/SGpp/trunk/lib/jsgpp directory.

Third, you need to set environment variables to specific values:
\code
export LD_LIBRARY_PATH="/path/to/SGpp/trunk/lib/sgpp:/path/to/SGpp/trunk/lib/alglib:/path/to/SGpp/trunk/lib/jsgpp"
export LD_PRELOAD="/usr/lib/x86_64-linux-gnu/libstdc++.so.6"
\endcode
The first line tells MATLAB where to find the necessary libraries.
The second line causes loading the system version of \c libstdc++
and not MATLAB's version
(for more info on these two lines,
see \ref code_examples_quick_start).

Fourth, after setting the environment variables, you can start MATLAB.
In MATLAB, run
\code
javaaddpath('/path/to/SGpp/trunk/lib/jsgpp/jsgpp.jar');
javaaddpath('/path/to/SGpp/trunk/optimization/examples/ExampleFunction.jar');
\endcode
to add both the jsgpp library and our example objective function to
MATLAB's Java search path.
Now, you should be able to run the MATLAB example.
When run, the example gives (nearly) the same output as the C++ example.

*/
