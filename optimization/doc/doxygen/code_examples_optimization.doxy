/**
\page code_examples_optimization Short Code Examples: Optimization

On this page, we look at an example application of the SGPP::optimization
module.
The example is written in C++.
However, as the module is also incorporated in pysgpp, one can also use
the Python interface.
Via the Java interface (jsgpp), the module should be usable in MATLAB
if compiled without support for Armadillo and UMFPACK
(see below for instructions).

\section code_examples_optimization_cpp C++

Below, you can find the whole C++ example.
The example minimizes a specific function \f$f\colon [0, 1]^2 \to \mathbb{R}\f$
using sparse grids and the SGPP::optimization module.
For the basic structure of the optimization, please see
\ref module_optimization.

\include c++_example.cpp

We will now discuss the example in some detail line by line.

\dontinclude c++_example.cpp
\until sgpp_optimization.hpp

We need to include the headers for the SGPP::base and the SGPP::optimization
module.

\until };

The function \f$f\colon [0, 1]^d \to \mathbb{R}\f$ to be minimized
is called "objective function" and has to derive from
SGPP::optimization::ObjectiveFunction.
In the constructor, we give the dimensionality of the domain
(in this case \f$d = 2\f$).
The eval method evaluates the objective function and returns the function
value \f$f(\vec{x})\f$ for a given point \f$\vec{x} \in [0, 1]^d\f$.
The clone method returns a std::unique_ptr to a clone of the object
and is used for parallelization (in case eval is not thread-safe).

\until f, grid, N, gamma

First, we define a grid with modified B-spline basis functions and
an iterative grid generator, which can generate the grid adaptively.

\until }

With the iterative grid generator, we generate adaptively a sparse grid.

\until }

Then, we hierarchize the function values to get hierarchical B-spline
coefficients of the B-spline sparse grid interpolant
\f$\tilde{f}\colon [0, 1]^d \to \mathbb{R}\f$.

\until ftX0

We define the interpolant \f$\tilde{f}\f$ and its gradient
\f$\nabla\tilde{f}\f$ for use with the gradient method (steepest descent).
Of course, one can also use other optimization algorithms from
SGPP::optimization::optimizer.

\until ft(x0)

The gradient method needs a starting point.
We use a point of our adaptively generated sparse grid as starting point.
More specifically, we use the point with the smallest
(most promising) function value and save it in x0.

\until ft(xOpt)

We apply the gradient method and print the results.

\until }

For comparison, we apply the classical gradient-free Nelder-Mead method
directly to the objective function \f$f\f$.

The example program outputs the following results:
\verbinclude all_example.output.txt

We see that both the gradient-based optimization of the smooth sparse grid
interpolant and the gradient-free optimization of the objective function
find reasonable approximations of the minimum, which lies at
\f$(3\pi/16, 3\pi/14) \approx (0.58904862, 0.67319843)\f$.

The example can be compiled, linked, and executed by
@code
g++ -std=c++11 -O3 -I../../base/src -I../../optimization/src -L../../lib/sgpp -o c++_example c++_example.cpp -lsgppbase -lsgppoptimization
export LD_LIBRARY_PATH="/path/to/SGpp/trunk/lib/sgpp"
./c++_example
@endcode
For details see \ref code_examples_quick_start.

\section code_examples_optimization_python Python

The C++ code can be translated into the following Python code:
\include python_example.py

To run the Python example, you need to adjust the environment
variables \c LD_LIBRARY_PATH and \c PYTHONPATH accordingly
(see \ref code_examples_quick_start).
When run, the example gives (nearly) the same output as the C++ example.

\section code_examples_optimization_java Java

The C++ code can be translated into the following Java code:
\include java_example.java

The Java and MATLAB examples use this external class:
\include ExampleFunction.java

To run the Java example, you need to adjust the environment
variable \c LD_LIBRARY_PATH accordingly
(see \ref code_examples_quick_start).
When run, the example gives (nearly) the same output as the C++ example.

\section code_examples_optimization_matlab MATLAB

The C++ code can be translated into the following MATLAB code:
\include matlab_example.m

Please note that in order to get SGPP::optimization to work with MATLAB,
you have to disable support for Armadillo and UMFPACK when compiling SG++,
i.e. set USE_ARMADILLO and USE_UMFPACK to "no".
This is due to incompatible BLAS and LAPACK libraries
of Armadillo/UMFPACK and MATLAB
(MATLAB uses instead MKL versions of LAPACK and BLAS
with different pointer sizes of 64 bits).
You can somehow override MATLAB's choice of libraries with
the environmental variables BLAS_VERSION and LAPACK_VERSION,
but this is strongly discouraged as MATLAB itself may produce
unexpected wrong results (e.g., det [1 2; 3 4] = 2).
Static linking to Armadillo and UMFPACK would be
a possible solution to circumvent this problem.

To make SG++ usuable within MATLAB, please follow
\ref code_examples__matlab "the instructions provided here".
However, to run the MATLAB example, you additionally need to compile
the class \c ExampleFunction into a \c .jar file
(before starting MATLAB):
\code
javac -cp .:/path/to/SGpp/trunk/lib/jsgpp/jsgpp.jar ExampleFunction.java
jar -cf ExampleFunction.jar ExampleFunction.class
\endcode
After setting the environment variables and the
\c librarypath.txt correctly
(\ref code_examples__matlab "as described here"), you can start MATLAB.
In order to run the example, you need to add the example objective
function to MATLAB's Java search path via
\code
javaaddpath('/path/to/SGpp/trunk/optimization/examples/ExampleFunction.jar');
\endcode
(i.e., right after adding \c jsgpp.jar to MATLAB's Java path).
Now, you should be able to run the MATLAB example.
When run, the example gives (nearly) the same output as the C++ example.

*/
